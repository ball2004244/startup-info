name: Crawl Startup Data

on:
  push:
    branches:
      - master
  schedule:
    - cron: "0 */6 * * *"

permissions: write-all

jobs:
  crawl:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v3

      - name: Set up Python 3.10
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install ChromeDriver
        uses: nanasess/setup-chromedriver@v2

      - name: Install dependencies
        run: |
          python3 -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Crawl data
        run: |
          echo "Start crawling process"
          echo "! This process might take a while to crawl the data"
          python3 -u scraper.py
          echo "End crawling!"

      - name: Format data
        run: |
          echo "Start formatting crawled data"
          python3 -u csv_formatter.py
          echo "End formatting!"

      - name: Check if remote branch exists
        run: |
          git config --local user.email "nguyenminhtam7124@gmail.com"
          git config --local user.name "Tam Nguyen"
          git pull

          if git ls-remote --heads origin data | grep -q 'data'; then
            git stash
            git checkout data
            git stash pop

          else
            git checkout -b data
          fi

      - name: Commit and push to GitHub
        run: |
          git add temp final.csv
          if ! git diff-index --quiet HEAD --; then
            git commit -m "Update data"
            git push origin data
          else
            echo "No files to commit"
          fi
