name: Crawl Startup Data

on:
  schedule:
    - cron: "0 */6 * * *"

  push:
    branches:
      - master

permissions: write-all

jobs:
  get-data:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v3

      - name: Set up Python 3.10
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install ChromeDriver
        uses: nanasess/setup-chromedriver@v2

      - name: Install dependencies
        run: |
          python3 -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Crawl data
        run: |
          echo "Start crawling process"
          echo "! This process might take a while to crawl the data"
          python3 -u multi_scraper.py
          echo "End crawling!"

      - name: Format data
        run: |
          echo "Start formatting crawled data"
          python3 -u csv_formatter.py
          echo "End formatting!"
      
      - name: Update cache data to GitHub
        uses: EndBug/add-and-commit@v9
        with:
          author_name: Tam Nguyen
          author_email: nguyenminhtam7124@gmail.com
          message: Update cache
          add: "check_set.pkl"
          new_branch: cache

      - name: Commit and push to GitHub
        uses: EndBug/add-and-commit@v9
        with:
          author_name: Tam Nguyen
          author_email: nguyenminhtam7124@gmail.com
          message: Update data
          add: "temp final.csv"
          new_branch: data
          push: origin data --set-upstream --force
